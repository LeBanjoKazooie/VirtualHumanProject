{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeBanjoKazooie/VirtualHumanProject/blob/main/Copy_of_VHPTeam3v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project was developed by COMP 670 Team 3 Virtual Human Project and it is based on the project specifications from Professor Chu. The goal of the project is to develop a Virtual Human Advisor for Franklin University. This app was initially developed using streamlit and replicate documentation. Later it was ported to Google Colab using HuggingFace Llama2 and Transformers documentation. While re-running the application Google Colab Help, Warnings, and Examples were useful in adding Google Drive, swapfile support and making minor edits to improve the application. The LLM model used is Lama-2-7b-chat-hf which uses the LamaForCausalLM architecture and dtype of Float16."
      ],
      "metadata": {
        "id": "kjJ1SO0NxR6d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVr9tNl2ca1M"
      },
      "outputs": [],
      "source": [
        "%pip install llama-index\n",
        "%pip install transformers\n",
        "%pip install -i https://pypi.org/simple/ bitsandbytes\n",
        "%pip install llama-index-readers-web\n",
        "%pip install llama-index-llms-huggingface\n",
        "%pip install llama-index-embeddings-huggingface\n",
        "%pip install llama-index-program-openai\n",
        "%pip install llama-index-agent-openai\n",
        "%pip install InstructorEmbedding\n",
        "%pip install torch\n",
        "%pip install pytorch-cuda\n",
        "%pip install safetensors\n",
        "%pip install google.colab\n",
        "%pip install sentence transformers\n",
        "%pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "from google.colab import userdata\n",
        "import torch\n",
        "from transformers import BitsAndBytesConfig\n",
        "from llama_index.core.prompts import PromptTemplate\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.core import Settings\n",
        "from InstructorEmbedding import INSTRUCTOR\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core.indices import SummaryIndex\n",
        "from llama_index.core.response.notebook_utils import display_response\n",
        "import logging\n",
        "import sys\n",
        "import safetensors\n",
        "from torch.cuda import is_available\n",
        "from accelerate import infer_auto_device_map, init_empty_weights\n",
        "from transformers import LlamaForCausalLM, LlamaConfig, LlamaTokenizer\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from llama_index.core.evaluation import EvaluationResult\n",
        "from llama_index.core.node_parser import TextSplitter\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "import accelerate"
      ],
      "metadata": {
        "id": "fV-hcJRhcvcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOADING DOCUMENTS:\n",
        "Create a folder named data next to the sample_data folder and upload the pdf documents to the folder data"
      ],
      "metadata": {
        "id": "Ol7k-F8ewL6W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rnQeOfxeN3g"
      },
      "outputs": [],
      "source": [
        "documents = SimpleDirectoryReader(\"/content/data\").load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftOD5Bq-evbW"
      },
      "outputs": [],
      "source": [
        "hf_token = userdata.get('hf_token')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SWAPFILE in progress"
      ],
      "metadata": {
        "id": "Sp-dZJHNba-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!fallocate -l 64G /content/swapfile\n",
        "!chmod 600 /content/swapfile\n",
        "!mkswap /content/swapfile\n",
        "!swapon --all --verbose /content/swapfile\n",
        "!swapon --show\n",
        "!free -h"
      ],
      "metadata": {
        "id": "ki7xG976bbsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOADING LlamaForCasualLM model with offload"
      ],
      "metadata": {
        "id": "-Awg7ft2AAXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "config = LlamaConfig.from_pretrained(model_name, token=hf_token, use_cache=True)\n",
        "\n",
        "with init_empty_weights():\n",
        "  model = AutoModelForCausalLM.from_config(config)\n",
        "\n",
        "device_map = infer_auto_device_map(model)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  quantization_config = BitsAndBytesConfig(\n",
        "      load_in_4bit=True,\n",
        "      bnb_4bit_compute_dtype=torch.float16,\n",
        "      #bnb_4bit_quant_type=\"nf4\",\n",
        "      #bnb_4bit_use_double_quant=True,\n",
        "  )\n",
        "  model = LlamaForCausalLM.from_pretrained(model_name, device_map=\"auto\", token=hf_token, offload_folder=\"llama-disk-offload\", use_cache=True, quantization_config=quantization_config)\n",
        "else:\n",
        "  model = LlamaForCausalLM.from_pretrained(model_name, device_map=\"auto\", token=hf_token, offload_folder=\"llama-disk-offload\", use_cache=True)\n"
      ],
      "metadata": {
        "id": "7AejSDRPouBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CONFIGURING HuggingFaceLLM MODEL FROM LlamaForCasualLM model: Load Llm model configuration. The maximum number of tokens on the input context is 2000.  The maximum number of response tokens is set to 512."
      ],
      "metadata": {
        "id": "dFGduyjjwcQS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCHk9piHe3M6"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "  quantization_config = BitsAndBytesConfig(\n",
        "      load_in_4bit=True,\n",
        "      bnb_4bit_compute_dtype=torch.float16,\n",
        "      #bnb_4bit_quant_type=\"nf4\",\n",
        "      #bnb_4bit_use_double_quant=True,\n",
        "  )\n",
        "  hf_model = HuggingFaceLLM(\n",
        "      model = model,\n",
        "      model_name=model_name,\n",
        "      tokenizer_name=model_name,\n",
        "      query_wrapper_prompt=PromptTemplate(\"<s> [INST] {query_str} [/INST] \"),\n",
        "      context_window=1024,\n",
        "      model_kwargs={\"token\": hf_token, \"quantization_config\": quantization_config},\n",
        "      tokenizer_kwargs={\"token\": hf_token},\n",
        "      device_map=\"auto\",\n",
        "      max_new_tokens=512,\n",
        "  )\n",
        "else:\n",
        "  hf_model = HuggingFaceLLM(\n",
        "      model = model,\n",
        "      model_name=model_name,\n",
        "      tokenizer_name=model_name,\n",
        "      query_wrapper_prompt=PromptTemplate(\"<s> [INST] {query_str} [/INST] \"),\n",
        "      context_window=1024,\n",
        "      tokenizer_kwargs={\"token\": hf_token},\n",
        "      device_map=\"auto\",\n",
        "      max_new_tokens=512,\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "INDEXING:\n",
        "Generate Vector index, embedings and transformations for Vector Search"
      ],
      "metadata": {
        "id": "nynD7EUVejVa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5DONLhQfnvU"
      },
      "outputs": [],
      "source": [
        "llama_embed_model = 'text-embedding-ada-002'\n",
        "embed_model = 'local:hkunlp/instructor-large'\n",
        "Settings.llm = hf_model\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  Settings.chunk_size = 512\n",
        "  Settings.chunk_overlap = 20\n",
        "  Settings.text_splitter = SentenceSplitter(chunk_size=512)\n",
        "  Settings.transformations = [Settings.text_splitter]\n",
        "else:\n",
        "  Settings.chunk_size = 512\n",
        "  Settings.chunk_overlap = 20\n",
        "  Settings.text_splitter = SentenceSplitter(chunk_size=512)\n",
        "  Settings.transformations = [Settings.text_splitter]\n",
        "\n",
        "Settings.embed_model=embed_model\n",
        "embeddings = Settings.embed_model(documents)\n",
        "\n",
        "print(f\"Embeddings: {embeddings}\")\n",
        "transformations = Settings.transformations\n",
        "vector_index = VectorStoreIndex.from_documents(documents, embed_model=embed_model, transformations=transformations)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STORING in progress:\n",
        "Store vector embeddings and indexing"
      ],
      "metadata": {
        "id": "FRQOnT2ZmAb1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUERING: The chatbot is primarily used for questions about Franklin University."
      ],
      "metadata": {
        "id": "9TqOJPmSwxe_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfbiDEkPi1WN"
      },
      "outputs": [],
      "source": [
        "#query_engine = vector_index.as_query_engine(response_mode=\"compact\")\n",
        "#prompt = str(input(\"Ask me a question about Franklin University!  \"))\n",
        "#response = query_engine.query(\"What are Ohio CPA Examination requirements?\")\n",
        "\n",
        "chat_engine = vector_index.as_chat_engine(chat_mode=\"context\",response_mode=\"compact\",max_new_tokens=512,\n",
        "                                          system_prompt=(\"You are a chatbot, able to have normal interactions, as well as talk about Franklin University\"),\n",
        "                                          llm=hf_model\n",
        "                                          )\n",
        "prompt = str(input(\"Ask me a question about Franklin University!  \"))\n",
        "response = chat_engine.chat(prompt)\n",
        "\n",
        "display_response(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EVALUATION in progress:\n",
        "Store querying prompt and responses for evaluation."
      ],
      "metadata": {
        "id": "OKdqJeKamLi1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index-vector-stores-qdrant qdrant_client\n",
        "!pip install fastembed\n"
      ],
      "metadata": {
        "id": "9Eph5RvmIUSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
        "from qdrant_client import QdrantClient\n",
        "from llama_index.core import StorageContext, VectorStoreIndex\n",
        "\n",
        "qdrant_client = QdrantClient(\n",
        "    url=\"https://4f411c47-4cba-4085-810c-dafbb4ca3bc3.us-east4-0.gcp.cloud.qdrant.io:6333\",\n",
        "    api_key=\"b9cWve2imbA9NJafCxUmiAAevxyjgcbns67-LRPhmTtSAkUf38qHxw\",\n",
        ")\n",
        "\n",
        "vector_store = QdrantVectorStore(\n",
        "    client=qdrant_client,\n",
        "    collection_name=\"mycollection\",\n",
        "    enable_hybrid=True,\n",
        "    batch_size=20\n",
        ")\n",
        "\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents=documents,\n",
        "    storage_context=storage_context\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "oiKDQuDnIkLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
        "import qdrant_client\n",
        "\n",
        "qdrant_client = QdrantClient(\n",
        "    url=\"https://4f411c47-4cba-4085-810c-dafbb4ca3bc3.us-east4-0.gcp.cloud.qdrant.io:6333\",\n",
        "    api_key=\"b9cWve2imbA9NJafCxUmiAAevxyjgcbns67-LRPhmTtSAkUf38qHxw\",\n",
        ")\n",
        "\n",
        "vector_store = QdrantVectorStore(client=qdrant_client, collection_name=\"mycollection\", enable_hybrid=True)\n",
        "index = VectorStoreIndex.from_vector_store(vector_store=vector_store)"
      ],
      "metadata": {
        "id": "jSipDTt0SSjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = index.as_query_engine(response_mode=\"compact\")\n",
        "prompt = str(input(\"Ask me a question about Franklin University!  \"))\n",
        "response = query_engine.query(prompt)\n",
        "\n",
        "display_response(response)"
      ],
      "metadata": {
        "id": "T63Xw5X1VjkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit"
      ],
      "metadata": {
        "id": "LGpAkJRmJUe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "\n",
        "st.write('Hello, *World!* :sunglasses:')"
      ],
      "metadata": {
        "id": "ncdrURbxMe_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel"
      ],
      "metadata": {
        "id": "88jXqBlVNM39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &>/content/logs.txt &\n"
      ],
      "metadata": {
        "id": "QVdeL-ScNTsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "print(\"Password/Enpoint IP for localtunnel is:\",urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))"
      ],
      "metadata": {
        "id": "TdFTPZsoQgXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npx localtunnel --port 8501\n"
      ],
      "metadata": {
        "id": "nTcaO7RgNW4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "opDE2eXuH3L_"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}